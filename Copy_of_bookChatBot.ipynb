{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QmGUhgr54efa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ri31tzAx4lK0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "f8OTNdi15vdk",
        "outputId": "f79f9be6-d7b4-4094-85a3-0815e68d6455"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>categories</th>\n",
              "      <th>description</th>\n",
              "      <th>published_year</th>\n",
              "      <th>average_rating</th>\n",
              "      <th>num_pages</th>\n",
              "      <th>ratings_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gilead</td>\n",
              "      <td>Marilynne Robinson</td>\n",
              "      <td>Fiction</td>\n",
              "      <td>A NOVEL THAT READERS and critics have been eag...</td>\n",
              "      <td>2004.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>247.0</td>\n",
              "      <td>361.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Spider's Web</td>\n",
              "      <td>Charles Osborne;Agatha Christie</td>\n",
              "      <td>Detective and mystery stories</td>\n",
              "      <td>A new 'Christie for Christmas' -- a full-lengt...</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>3.83</td>\n",
              "      <td>241.0</td>\n",
              "      <td>5164.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The One Tree</td>\n",
              "      <td>Stephen R. Donaldson</td>\n",
              "      <td>American fiction</td>\n",
              "      <td>Volume Two of Stephen Donaldson's acclaimed se...</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>3.97</td>\n",
              "      <td>479.0</td>\n",
              "      <td>172.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rage of angels</td>\n",
              "      <td>Sidney Sheldon</td>\n",
              "      <td>Fiction</td>\n",
              "      <td>A memorable, mesmerizing heroine Jennifer -- b...</td>\n",
              "      <td>1993.0</td>\n",
              "      <td>3.93</td>\n",
              "      <td>512.0</td>\n",
              "      <td>29532.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Four Loves</td>\n",
              "      <td>Clive Staples Lewis</td>\n",
              "      <td>Christian life</td>\n",
              "      <td>Lewis' work on the nature of love divides love...</td>\n",
              "      <td>2002.0</td>\n",
              "      <td>4.15</td>\n",
              "      <td>170.0</td>\n",
              "      <td>33684.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            title                          authors  \\\n",
              "0          Gilead               Marilynne Robinson   \n",
              "1    Spider's Web  Charles Osborne;Agatha Christie   \n",
              "2    The One Tree             Stephen R. Donaldson   \n",
              "3  Rage of angels                   Sidney Sheldon   \n",
              "4  The Four Loves              Clive Staples Lewis   \n",
              "\n",
              "                      categories  \\\n",
              "0                        Fiction   \n",
              "1  Detective and mystery stories   \n",
              "2               American fiction   \n",
              "3                        Fiction   \n",
              "4                 Christian life   \n",
              "\n",
              "                                         description  published_year  \\\n",
              "0  A NOVEL THAT READERS and critics have been eag...          2004.0   \n",
              "1  A new 'Christie for Christmas' -- a full-lengt...          2000.0   \n",
              "2  Volume Two of Stephen Donaldson's acclaimed se...          1982.0   \n",
              "3  A memorable, mesmerizing heroine Jennifer -- b...          1993.0   \n",
              "4  Lewis' work on the nature of love divides love...          2002.0   \n",
              "\n",
              "   average_rating  num_pages  ratings_count  \n",
              "0            3.85      247.0          361.0  \n",
              "1            3.83      241.0         5164.0  \n",
              "2            3.97      479.0          172.0  \n",
              "3            3.93      512.0        29532.0  \n",
              "4            4.15      170.0        33684.0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(columns=['isbn13', 'isbn10', 'subtitle', 'thumbnail'], inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcXuZyYd5zzf",
        "outputId": "f72f07e4-8d5a-439f-a365-420ededa0f91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "categories\n",
              "Fiction                          2588\n",
              "Juvenile Fiction                  538\n",
              "Biography & Autobiography         401\n",
              "History                           264\n",
              "Literary Criticism                166\n",
              "Philosophy                        160\n",
              "Comics & Graphic Novels           159\n",
              "Religion                          137\n",
              "Drama                             132\n",
              "Juvenile Nonfiction               116\n",
              "Poetry                             79\n",
              "Science                            71\n",
              "Literary Collections               71\n",
              "Business & Economics               67\n",
              "Social Science                     60\n",
              "Performing Arts                    50\n",
              "Cooking                            47\n",
              "Art                                46\n",
              "Body, Mind & Spirit                44\n",
              "Travel                             43\n",
              "Psychology                         43\n",
              "Computers                          42\n",
              "Self-Help                          38\n",
              "Political Science                  36\n",
              "Family & Relationships             34\n",
              "Language Arts & Disciplines        33\n",
              "Humor                              32\n",
              "Health & Fitness                   32\n",
              "Children's stories                 28\n",
              "Education                          27\n",
              "Medical                            25\n",
              "Nature                             24\n",
              "Adventure stories                  23\n",
              "Games                              22\n",
              "English fiction                    19\n",
              "Music                              19\n",
              "Sports & Recreation                17\n",
              "Detective and mystery stories      17\n",
              "Fantasy fiction                    16\n",
              "American fiction                   15\n",
              "True Crime                         12\n",
              "Foreign Language Study             12\n",
              "Science fiction                    12\n",
              "Law                                11\n",
              "Young Adult Fiction                11\n",
              "Photography                        11\n",
              "Reference                          11\n",
              "Authors, American                  10\n",
              "Architecture                       10\n",
              "England                             9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['categories'].value_counts().head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rvVOVieU56AD"
      },
      "outputs": [],
      "source": [
        "genres = ['Fiction', 'Juvenile Fiction', 'Biography & Autobiography', 'History', 'Literary Criticism', 'Philosophy', 'Comics & Graphic Novels', 'Religion',\n",
        "          'Drama', 'Juvenile Nonfiction', 'Poetry', 'Science', 'Literary Collections', 'Business & Economics', 'Social Science', 'Performing Arts', 'Cooking',\n",
        "          'Art', 'Body, Mind & Spirit', 'Travel', 'Psychology', 'Computers', 'Self-Help', 'Political Science', 'Family & Relationships', 'Language Arts & Disciplines',\n",
        "          'Humor', 'Health & Fitness', \"Children's stories\", 'Education', 'Medical', 'Nature', 'Adventure stories', 'Games', 'English fiction', 'Music', 'Sports & Recreation',\n",
        "          'Detective and mystery stories', 'Fantasy fiction', 'American fiction', 'True Crime', 'Foreign Language Study', 'Science fiction', 'Law', 'Young Adult Fiction',\n",
        "          'Photography', 'Architecture']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P-PhXpjG56XO"
      },
      "outputs": [],
      "source": [
        "titles = []\n",
        "describs = []\n",
        "reates = []\n",
        "for g in genres:\n",
        "    titles.append(df[df['categories'] == g]['title'].values)\n",
        "    describs.append(df[df['categories'] == g]['description'].values)\n",
        "    reates.append(df[df['categories'] == g]['average_rating'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_qEWar-y6Ef5"
      },
      "outputs": [],
      "source": [
        "intents = []\n",
        "res = []\n",
        "\n",
        "for i in range(len(titles)):\n",
        "    for j in range(10):\n",
        "        res.append({\"Book\":titles[i][j], \"Feedback\":describs[i][j], \"Rate\":reates[i][j]})\n",
        "\n",
        "        d = {\n",
        "        \"tag\": f\"{genres[i]}\",\n",
        "        \"patterns\": [f\"{genres[i]}\"],\n",
        "        \"responses\": [f\"{res}\"]}\n",
        "\n",
        "        intents.append(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qchmH03q6IcH"
      },
      "outputs": [],
      "source": [
        "dic = {'intents':intents}\n",
        "\n",
        "# Serializing json\n",
        "json_object = json.dumps(dic)\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(\"intents.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isZs40bF6MFx",
        "outputId": "fe5c0d56-a38c-48b4-a56d-cd100a0dbafb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      9\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Dropout\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Import everything from /api/ into keras.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# Import * ignores names start with \"_\".\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add everything in /api/ to the module search path.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasVariable\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[0;32m      7\u001b[0m BOOL_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m      8\u001b[0m INT_TYPES \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\variables.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasVariable\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Represents a backend-agnostic variable in Keras.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    A `Variable` acts as a container for state. It holds a tensor value and can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_interactive_logging\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_interactive_logging_enabled\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_visualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_visualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumerical_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\model_visualization.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_nested\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\tree\\tree_api.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dmtree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\tree\\optree_impl.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[0;32m     19\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[0;32m     20\u001b[0m         ListWrapper,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[0;32m     23\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\__init__.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5906\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   5900\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_to_proto_function\u001b[39m(\n\u001b[0;32m   5905\u001b[0m     collection_name,\n\u001b[1;32m-> 5906\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCallable\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m   5907\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the to_proto function for collection_name.\"\"\"\u001b[39;00m\n\u001b[0;32m   5908\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:243\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:316\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@_tp_cache\u001b[39m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parameters):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:433\u001b[0m, in \u001b[0;36mOptional\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optional type.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mOptional[X] is equivalent to Union[X, None].\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m arg \u001b[38;5;241m=\u001b[39m _type_check(parameters, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires a single type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:243\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:316\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@_tp_cache\u001b[39m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parameters):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:421\u001b[0m, in \u001b[0;36mUnion\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m    419\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(_type_check(p, msg) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[1;32m--> 421\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43m_remove_dups_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parameters[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:215\u001b[0m, in \u001b[0;36m_remove_dups_flatten\u001b[1;34m(parameters)\u001b[0m\n\u001b[0;32m    213\u001b[0m         params\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Weed out strict duplicates, preserving the first of each occurrence.\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m all_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_params) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(params):\n\u001b[0;32m    217\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sX-WD8Up6UkM"
      },
      "outputs": [],
      "source": [
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ftkM1EcO6cXk"
      },
      "outputs": [],
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJYtjY1v6gXS",
        "outputId": "b0e91daa-60ca-4b11-8608-68478a5ba578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "470 documents\n",
            "66 unique lemmatized words\n",
            "47 classes ['Adventure stories', 'American fiction', 'Architecture', 'Art', 'Biography & Autobiography', 'Body, Mind & Spirit', 'Business & Economics', \"Children's stories\", 'Comics & Graphic Novels', 'Computers', 'Cooking', 'Detective and mystery stories', 'Drama', 'Education', 'English fiction', 'Family & Relationships', 'Fantasy fiction', 'Fiction', 'Foreign Language Study', 'Games', 'Health & Fitness', 'History', 'Humor', 'Juvenile Fiction', 'Juvenile Nonfiction', 'Language Arts & Disciplines', 'Law', 'Literary Collections', 'Literary Criticism', 'Medical', 'Music', 'Nature', 'Performing Arts', 'Philosophy', 'Photography', 'Poetry', 'Political Science', 'Psychology', 'Religion', 'Science', 'Science fiction', 'Self-Help', 'Social Science', 'Sports & Recreation', 'Travel', 'True Crime', 'Young Adult Fiction']\n"
          ]
        }
      ],
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        # take each word and tokenize it\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "\n",
        "        # adding documents\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # adding classes to our class list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(words), \"unique lemmatized words\")\n",
        "print (len(classes), \"classes\", classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "86GVuDQI6021"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Find the maximum length of any list in the training list\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bag) \u001b[38;5;28;01mfor\u001b[39;00m bag, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Pad all lists in the training list to the maximum length\u001b[39;00m\n\u001b[0;32m      5\u001b[0m padded_training \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     (np\u001b[38;5;241m.\u001b[39mpad(bag, (\u001b[38;5;241m0\u001b[39m, max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(bag)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m), output_row)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bag, output_row \u001b[38;5;129;01min\u001b[39;00m training\n\u001b[0;32m      8\u001b[0m ]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
          ]
        }
      ],
      "source": [
        "# Find the maximum length of any list in the training list\n",
        "max_length = max(len(bag) for bag, _ in training)\n",
        "\n",
        "# Pad all lists in the training list to the maximum length\n",
        "padded_training = [\n",
        "    (np.pad(bag, (0, max_length - len(bag)), 'constant'), output_row)\n",
        "    for bag, output_row in training\n",
        "]\n",
        "\n",
        "# Convert the padded training variable to a NumPy array\n",
        "training = np.array(padded_training, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PYQIGEb8JrEF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'padded_training' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# shuffle our features and turn into np.array\u001b[39;00m\n\u001b[0;32m     25\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(training)\n\u001b[1;32m---> 26\u001b[0m training \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mpadded_training\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# create train and test lists. X - patterns, Y - intents\u001b[39;00m\n\u001b[0;32m     29\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training[:, \u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'padded_training' is not defined"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "\n",
        "    # initializing bag of words\n",
        "    bag = []\n",
        "\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(padded_training, dtype=object)\n",
        "\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjA88DqrJPrG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elpl1ps5JPjK",
        "outputId": "60339c26-a642-44fe-8314-6aa537049de7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ],
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FShPVUWaJPvA",
        "outputId": "2577bcc3-28b8-46b8-8f09-1f18e1681323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "94/94 [==============================] - 1s 3ms/step - loss: 3.8091 - accuracy: 0.0617\n",
            "Epoch 2/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3.5080 - accuracy: 0.1638\n",
            "Epoch 3/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3.0176 - accuracy: 0.2660\n",
            "Epoch 4/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 2.4221 - accuracy: 0.3979\n",
            "Epoch 5/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.8433 - accuracy: 0.5383\n",
            "Epoch 6/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.3241 - accuracy: 0.6617\n",
            "Epoch 7/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.0678 - accuracy: 0.7255\n",
            "Epoch 8/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.9024 - accuracy: 0.7362\n",
            "Epoch 9/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.7585 - accuracy: 0.7936\n",
            "Epoch 10/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.8319\n",
            "Epoch 11/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.8383\n",
            "Epoch 12/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.8766\n",
            "Epoch 13/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.8383\n",
            "Epoch 14/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8638\n",
            "Epoch 15/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8936\n",
            "Epoch 16/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.9021\n",
            "Epoch 17/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8702\n",
            "Epoch 18/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8915\n",
            "Epoch 19/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8936\n",
            "Epoch 20/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8979\n",
            "Epoch 21/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.9128\n",
            "Epoch 22/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.9234\n",
            "Epoch 23/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9085\n",
            "Epoch 24/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9277\n",
            "Epoch 25/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9383\n",
            "Epoch 26/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2466 - accuracy: 0.9362\n",
            "Epoch 27/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9447\n",
            "Epoch 28/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9489\n",
            "Epoch 29/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9404\n",
            "Epoch 30/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9404\n",
            "Epoch 31/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9447\n",
            "Epoch 32/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9213\n",
            "Epoch 33/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.9553\n",
            "Epoch 34/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9489\n",
            "Epoch 35/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9319\n",
            "Epoch 36/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9447\n",
            "Epoch 37/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9574\n",
            "Epoch 38/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9553\n",
            "Epoch 39/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1617 - accuracy: 0.9553\n",
            "Epoch 40/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1800 - accuracy: 0.9447\n",
            "Epoch 41/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9511\n",
            "Epoch 42/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9532\n",
            "Epoch 43/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9489\n",
            "Epoch 44/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9447\n",
            "Epoch 45/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9511\n",
            "Epoch 46/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1152 - accuracy: 0.9702\n",
            "Epoch 47/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1653 - accuracy: 0.9511\n",
            "Epoch 48/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9340\n",
            "Epoch 49/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.9660\n",
            "Epoch 50/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9617\n",
            "Epoch 51/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9553\n",
            "Epoch 52/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9638\n",
            "Epoch 53/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9596\n",
            "Epoch 54/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1082 - accuracy: 0.9638\n",
            "Epoch 55/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9553\n",
            "Epoch 56/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1195 - accuracy: 0.9553\n",
            "Epoch 57/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1308 - accuracy: 0.9574\n",
            "Epoch 58/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9766\n",
            "Epoch 59/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1324 - accuracy: 0.9468\n",
            "Epoch 60/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.9660\n",
            "Epoch 61/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9638\n",
            "Epoch 62/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9596\n",
            "Epoch 63/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9745\n",
            "Epoch 64/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9617\n",
            "Epoch 65/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9702\n",
            "Epoch 66/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9553\n",
            "Epoch 67/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9426\n",
            "Epoch 68/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1119 - accuracy: 0.9596\n",
            "Epoch 69/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.9404\n",
            "Epoch 70/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9660\n",
            "Epoch 71/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9532\n",
            "Epoch 72/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9638\n",
            "Epoch 73/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9723\n",
            "Epoch 74/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9404\n",
            "Epoch 75/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9702\n",
            "Epoch 76/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1551 - accuracy: 0.9404\n",
            "Epoch 77/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 0.9468\n",
            "Epoch 78/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9489\n",
            "Epoch 79/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9617\n",
            "Epoch 80/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.9660\n",
            "Epoch 81/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9617\n",
            "Epoch 82/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9468\n",
            "Epoch 83/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9617\n",
            "Epoch 84/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1094 - accuracy: 0.9596\n",
            "Epoch 85/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9681\n",
            "Epoch 86/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9511\n",
            "Epoch 87/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9617\n",
            "Epoch 88/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9638\n",
            "Epoch 89/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9532\n",
            "Epoch 90/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9532\n",
            "Epoch 91/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9745\n",
            "Epoch 92/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9702\n",
            "Epoch 93/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9702\n",
            "Epoch 94/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9574\n",
            "Epoch 95/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9660\n",
            "Epoch 96/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0860 - accuracy: 0.9809\n",
            "Epoch 97/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9596\n",
            "Epoch 98/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.9809\n",
            "Epoch 99/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.9723\n",
            "Epoch 100/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9766\n",
            "Epoch 101/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9638\n",
            "Epoch 102/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9638\n",
            "Epoch 103/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0871 - accuracy: 0.9681\n",
            "Epoch 104/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9574\n",
            "Epoch 105/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9638\n",
            "Epoch 106/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9489\n",
            "Epoch 107/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9660\n",
            "Epoch 108/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9596\n",
            "Epoch 109/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.9660\n",
            "Epoch 110/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9702\n",
            "Epoch 111/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.9553\n",
            "Epoch 112/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9638\n",
            "Epoch 113/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9660\n",
            "Epoch 114/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9745\n",
            "Epoch 115/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9681\n",
            "Epoch 116/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9553\n",
            "Epoch 117/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9851\n",
            "Epoch 118/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9553\n",
            "Epoch 119/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9766\n",
            "Epoch 120/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9723\n",
            "Epoch 121/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9745\n",
            "Epoch 122/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.9681\n",
            "Epoch 123/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9723\n",
            "Epoch 124/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9872\n",
            "Epoch 125/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9702\n",
            "Epoch 126/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9617\n",
            "Epoch 127/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9532\n",
            "Epoch 128/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9553\n",
            "Epoch 129/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9681\n",
            "Epoch 130/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9702\n",
            "Epoch 131/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9532\n",
            "Epoch 132/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9787\n",
            "Epoch 133/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9660\n",
            "Epoch 134/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9766\n",
            "Epoch 135/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9638\n",
            "Epoch 136/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9660\n",
            "Epoch 137/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9681\n",
            "Epoch 138/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.9596\n",
            "Epoch 139/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.9638\n",
            "Epoch 140/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9723\n",
            "Epoch 141/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9745\n",
            "Epoch 142/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9681\n",
            "Epoch 143/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9723\n",
            "Epoch 144/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9681\n",
            "Epoch 145/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.9468\n",
            "Epoch 146/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1162 - accuracy: 0.9574\n",
            "Epoch 147/500\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0996 - accuracy: 0.9745\n",
            "Epoch 148/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9681\n",
            "Epoch 149/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.9702\n",
            "Epoch 150/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9723\n",
            "Epoch 151/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9787\n",
            "Epoch 152/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9723\n",
            "Epoch 153/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9660\n",
            "Epoch 154/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9702\n",
            "Epoch 155/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.9745\n",
            "Epoch 156/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9723\n",
            "Epoch 157/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9723\n",
            "Epoch 158/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1096 - accuracy: 0.9660\n",
            "Epoch 159/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9660\n",
            "Epoch 160/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9702\n",
            "Epoch 161/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0815 - accuracy: 0.9702\n",
            "Epoch 162/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9766\n",
            "Epoch 163/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9830\n",
            "Epoch 164/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9766\n",
            "Epoch 165/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9681\n",
            "Epoch 166/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0938 - accuracy: 0.9660\n",
            "Epoch 167/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9660\n",
            "Epoch 168/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9660\n",
            "Epoch 169/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0856 - accuracy: 0.9723\n",
            "Epoch 170/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9702\n",
            "Epoch 171/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9660\n",
            "Epoch 172/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9787\n",
            "Epoch 173/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9787\n",
            "Epoch 174/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9745\n",
            "Epoch 175/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9574\n",
            "Epoch 176/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9723\n",
            "Epoch 177/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9638\n",
            "Epoch 178/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9681\n",
            "Epoch 179/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9702\n",
            "Epoch 180/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0929 - accuracy: 0.9745\n",
            "Epoch 181/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9766\n",
            "Epoch 182/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9745\n",
            "Epoch 183/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9766\n",
            "Epoch 184/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9745\n",
            "Epoch 185/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9468\n",
            "Epoch 186/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9745\n",
            "Epoch 187/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9617\n",
            "Epoch 188/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9681\n",
            "Epoch 189/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9638\n",
            "Epoch 190/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.9766\n",
            "Epoch 191/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9638\n",
            "Epoch 192/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9745\n",
            "Epoch 193/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9681\n",
            "Epoch 194/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9660\n",
            "Epoch 195/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9723\n",
            "Epoch 196/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9830\n",
            "Epoch 197/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9787\n",
            "Epoch 198/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9681\n",
            "Epoch 199/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9702\n",
            "Epoch 200/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9766\n",
            "Epoch 201/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9723\n",
            "Epoch 202/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0965 - accuracy: 0.9553\n",
            "Epoch 203/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9766\n",
            "Epoch 204/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0920 - accuracy: 0.9702\n",
            "Epoch 205/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1042 - accuracy: 0.9681\n",
            "Epoch 206/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9660\n",
            "Epoch 207/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9723\n",
            "Epoch 208/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9766\n",
            "Epoch 209/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9660\n",
            "Epoch 210/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9766\n",
            "Epoch 211/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9681\n",
            "Epoch 212/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9723\n",
            "Epoch 213/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9745\n",
            "Epoch 214/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9617\n",
            "Epoch 215/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9638\n",
            "Epoch 216/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1014 - accuracy: 0.9660\n",
            "Epoch 217/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1086 - accuracy: 0.9702\n",
            "Epoch 218/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9745\n",
            "Epoch 219/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9787\n",
            "Epoch 220/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9596\n",
            "Epoch 221/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9574\n",
            "Epoch 222/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9830\n",
            "Epoch 223/500\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9851\n",
            "Epoch 224/500\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0809 - accuracy: 0.9702\n",
            "Epoch 225/500\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0727 - accuracy: 0.9745\n",
            "Epoch 226/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9660\n",
            "Epoch 227/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9809\n",
            "Epoch 228/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9723\n",
            "Epoch 229/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9872\n",
            "Epoch 230/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9702\n",
            "Epoch 231/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9702\n",
            "Epoch 232/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1093 - accuracy: 0.9681\n",
            "Epoch 233/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9745\n",
            "Epoch 234/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9723\n",
            "Epoch 235/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9830\n",
            "Epoch 236/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9830\n",
            "Epoch 237/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9681\n",
            "Epoch 238/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.9894\n",
            "Epoch 239/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9766\n",
            "Epoch 240/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.9872\n",
            "Epoch 241/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9766\n",
            "Epoch 242/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1152 - accuracy: 0.9681\n",
            "Epoch 243/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9915\n",
            "Epoch 244/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9766\n",
            "Epoch 245/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9596\n",
            "Epoch 246/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9809\n",
            "Epoch 247/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9617\n",
            "Epoch 248/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9766\n",
            "Epoch 249/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9745\n",
            "Epoch 250/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9830\n",
            "Epoch 251/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9681\n",
            "Epoch 252/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9702\n",
            "Epoch 253/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9702\n",
            "Epoch 254/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9766\n",
            "Epoch 255/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.9681\n",
            "Epoch 256/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9574\n",
            "Epoch 257/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9638\n",
            "Epoch 258/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9766\n",
            "Epoch 259/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.9702\n",
            "Epoch 260/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9723\n",
            "Epoch 261/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9809\n",
            "Epoch 262/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9702\n",
            "Epoch 263/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.9787\n",
            "Epoch 264/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9766\n",
            "Epoch 265/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9766\n",
            "Epoch 266/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9723\n",
            "Epoch 267/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9702\n",
            "Epoch 268/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9723\n",
            "Epoch 269/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.9723\n",
            "Epoch 270/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9809\n",
            "Epoch 271/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9702\n",
            "Epoch 272/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9809\n",
            "Epoch 273/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9851\n",
            "Epoch 274/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9660\n",
            "Epoch 275/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9681\n",
            "Epoch 276/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9766\n",
            "Epoch 277/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0567 - accuracy: 0.9830\n",
            "Epoch 278/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9787\n",
            "Epoch 279/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9660\n",
            "Epoch 280/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0806 - accuracy: 0.9660\n",
            "Epoch 281/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9766\n",
            "Epoch 282/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0858 - accuracy: 0.9745\n",
            "Epoch 283/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9766\n",
            "Epoch 284/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9723\n",
            "Epoch 285/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9702\n",
            "Epoch 286/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9596\n",
            "Epoch 287/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9723\n",
            "Epoch 288/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9766\n",
            "Epoch 289/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9809\n",
            "Epoch 290/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9787\n",
            "Epoch 291/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9723\n",
            "Epoch 292/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9766\n",
            "Epoch 293/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9787\n",
            "Epoch 294/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9745\n",
            "Epoch 295/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9787\n",
            "Epoch 296/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9723\n",
            "Epoch 297/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0850 - accuracy: 0.9723\n",
            "Epoch 298/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1075 - accuracy: 0.9660\n",
            "Epoch 299/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1097 - accuracy: 0.9723\n",
            "Epoch 300/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9830\n",
            "Epoch 301/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9745\n",
            "Epoch 302/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9723\n",
            "Epoch 303/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9851\n",
            "Epoch 304/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9681\n",
            "Epoch 305/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1094 - accuracy: 0.9702\n",
            "Epoch 306/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9617\n",
            "Epoch 307/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9617\n",
            "Epoch 308/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0897 - accuracy: 0.9745\n",
            "Epoch 309/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9617\n",
            "Epoch 310/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.9702\n",
            "Epoch 311/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9617\n",
            "Epoch 312/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9702\n",
            "Epoch 313/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9681\n",
            "Epoch 314/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9745\n",
            "Epoch 315/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9702\n",
            "Epoch 316/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9681\n",
            "Epoch 317/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9787\n",
            "Epoch 318/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9809\n",
            "Epoch 319/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9702\n",
            "Epoch 320/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9702\n",
            "Epoch 321/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9660\n",
            "Epoch 322/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9787\n",
            "Epoch 323/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9830\n",
            "Epoch 324/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9787\n",
            "Epoch 325/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9766\n",
            "Epoch 326/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.9723\n",
            "Epoch 327/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9830\n",
            "Epoch 328/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9702\n",
            "Epoch 329/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9723\n",
            "Epoch 330/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9702\n",
            "Epoch 331/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9660\n",
            "Epoch 332/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9638\n",
            "Epoch 333/500\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0526 - accuracy: 0.9809\n",
            "Epoch 334/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9809\n",
            "Epoch 335/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9660\n",
            "Epoch 336/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9638\n",
            "Epoch 337/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9596\n",
            "Epoch 338/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9766\n",
            "Epoch 339/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9830\n",
            "Epoch 340/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9787\n",
            "Epoch 341/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9745\n",
            "Epoch 342/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9851\n",
            "Epoch 343/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9787\n",
            "Epoch 344/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0607 - accuracy: 0.9787\n",
            "Epoch 345/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1408 - accuracy: 0.9617\n",
            "Epoch 346/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9617\n",
            "Epoch 347/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9830\n",
            "Epoch 348/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9660\n",
            "Epoch 349/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9702\n",
            "Epoch 350/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9681\n",
            "Epoch 351/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9872\n",
            "Epoch 352/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9809\n",
            "Epoch 353/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9660\n",
            "Epoch 354/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9702\n",
            "Epoch 355/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9851\n",
            "Epoch 356/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9617\n",
            "Epoch 357/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9766\n",
            "Epoch 358/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1068 - accuracy: 0.9660\n",
            "Epoch 359/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9681\n",
            "Epoch 360/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9766\n",
            "Epoch 361/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9787\n",
            "Epoch 362/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9681\n",
            "Epoch 363/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9809\n",
            "Epoch 364/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9745\n",
            "Epoch 365/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9809\n",
            "Epoch 366/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9596\n",
            "Epoch 367/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9681\n",
            "Epoch 368/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9660\n",
            "Epoch 369/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 0.9660\n",
            "Epoch 370/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9702\n",
            "Epoch 371/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9766\n",
            "Epoch 372/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9809\n",
            "Epoch 373/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9745\n",
            "Epoch 374/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9596\n",
            "Epoch 375/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0806 - accuracy: 0.9830\n",
            "Epoch 376/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 0.9766\n",
            "Epoch 377/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9787\n",
            "Epoch 378/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.9894\n",
            "Epoch 379/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9723\n",
            "Epoch 380/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.9638\n",
            "Epoch 381/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9723\n",
            "Epoch 382/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9745\n",
            "Epoch 383/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0980 - accuracy: 0.9660\n",
            "Epoch 384/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9702\n",
            "Epoch 385/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9681\n",
            "Epoch 386/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9830\n",
            "Epoch 387/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9936\n",
            "Epoch 388/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9830\n",
            "Epoch 389/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9596\n",
            "Epoch 390/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9745\n",
            "Epoch 391/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9702\n",
            "Epoch 392/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9660\n",
            "Epoch 393/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0788 - accuracy: 0.9723\n",
            "Epoch 394/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9766\n",
            "Epoch 395/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9787\n",
            "Epoch 396/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.9660\n",
            "Epoch 397/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.9596\n",
            "Epoch 398/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.9745\n",
            "Epoch 399/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9617\n",
            "Epoch 400/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9745\n",
            "Epoch 401/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9638\n",
            "Epoch 402/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9681\n",
            "Epoch 403/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9681\n",
            "Epoch 404/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9851\n",
            "Epoch 405/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0821 - accuracy: 0.9766\n",
            "Epoch 406/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9809\n",
            "Epoch 407/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9809\n",
            "Epoch 408/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9681\n",
            "Epoch 409/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9702\n",
            "Epoch 410/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9745\n",
            "Epoch 411/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9830\n",
            "Epoch 412/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9872\n",
            "Epoch 413/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9745\n",
            "Epoch 414/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9723\n",
            "Epoch 415/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9766\n",
            "Epoch 416/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.9702\n",
            "Epoch 417/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9723\n",
            "Epoch 418/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1098 - accuracy: 0.9553\n",
            "Epoch 419/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9702\n",
            "Epoch 420/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.9787\n",
            "Epoch 421/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9787\n",
            "Epoch 422/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9787\n",
            "Epoch 423/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9830\n",
            "Epoch 424/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0858 - accuracy: 0.9745\n",
            "Epoch 425/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9723\n",
            "Epoch 426/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.9787\n",
            "Epoch 427/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9702\n",
            "Epoch 428/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9766\n",
            "Epoch 429/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9681\n",
            "Epoch 430/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9787\n",
            "Epoch 431/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9723\n",
            "Epoch 432/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9745\n",
            "Epoch 433/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9830\n",
            "Epoch 434/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9787\n",
            "Epoch 435/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9723\n",
            "Epoch 436/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.9723\n",
            "Epoch 437/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0936 - accuracy: 0.9660\n",
            "Epoch 438/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9681\n",
            "Epoch 439/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9809\n",
            "Epoch 440/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9702\n",
            "Epoch 441/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9766\n",
            "Epoch 442/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.9745\n",
            "Epoch 443/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9766\n",
            "Epoch 444/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1455 - accuracy: 0.9596\n",
            "Epoch 445/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9702\n",
            "Epoch 446/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9574\n",
            "Epoch 447/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9851\n",
            "Epoch 448/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9681\n",
            "Epoch 449/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9766\n",
            "Epoch 450/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.9766\n",
            "Epoch 451/500\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1613 - accuracy: 0.9489\n",
            "Epoch 452/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9723\n",
            "Epoch 453/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9702\n",
            "Epoch 454/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9894\n",
            "Epoch 455/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9702\n",
            "Epoch 456/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9830\n",
            "Epoch 457/500\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9809\n",
            "Epoch 458/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9766\n",
            "Epoch 459/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.9766\n",
            "Epoch 460/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9851\n",
            "Epoch 461/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9638\n",
            "Epoch 462/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9787\n",
            "Epoch 463/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0670 - accuracy: 0.9766\n",
            "Epoch 464/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9830\n",
            "Epoch 465/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 0.9809\n",
            "Epoch 466/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9723\n",
            "Epoch 467/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9723\n",
            "Epoch 468/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9809\n",
            "Epoch 469/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9787\n",
            "Epoch 470/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9511\n",
            "Epoch 471/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1012 - accuracy: 0.9702\n",
            "Epoch 472/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9809\n",
            "Epoch 473/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 0.9702\n",
            "Epoch 474/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9553\n",
            "Epoch 475/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9723\n",
            "Epoch 476/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9745\n",
            "Epoch 477/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9745\n",
            "Epoch 478/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9809\n",
            "Epoch 479/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9787\n",
            "Epoch 480/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9766\n",
            "Epoch 481/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9723\n",
            "Epoch 482/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9745\n",
            "Epoch 483/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9787\n",
            "Epoch 484/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9787\n",
            "Epoch 485/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9787\n",
            "Epoch 486/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9851\n",
            "Epoch 487/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9787\n",
            "Epoch 488/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9723\n",
            "Epoch 489/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9830\n",
            "Epoch 490/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9723\n",
            "Epoch 491/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9745\n",
            "Epoch 492/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1154 - accuracy: 0.9660\n",
            "Epoch 493/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9766\n",
            "Epoch 494/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9809\n",
            "Epoch 495/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9723\n",
            "Epoch 496/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9617\n",
            "Epoch 497/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9702\n",
            "Epoch 498/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9702\n",
            "Epoch 499/500\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9660\n",
            "Epoch 500/500\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9809\n",
            "model created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVEir49eJPyK"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "\n",
        "    return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvUdT1kyJP1I"
      },
      "outputs": [],
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "\n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYD6zMVaJP4C"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "\n",
        "    return return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mDcai1dJP6s"
      },
      "outputs": [],
      "source": [
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6zcYzzbLQlF"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "XOsZ041JLQtA",
        "outputId": "2734a37b-baaf-40c9-d018-878a6398f483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 112ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[{\\'Book\\': \\'Gilead\\', \\'Feedback\\': \\'A NOVEL THAT READERS and critics have been eagerly anticipating for over a decade, Gilead is an astonishingly imagined story of remarkable lives. John Ames is a preacher, the son of a preacher and the grandson (both maternal and paternal) of preachers. Its 1956 in Gilead, Iowa, towards the end of the Reverend Amess life, and he is absorbed in recording his familys story, a legacy for the young son he will never see grow up. Haunted by his grandfathers presence, John tells of the rift between his grandfather and his father: the elder, an angry visionary who fought for the abolitionist cause, and his son, an ardent pacifist. He is troubled, too, by his prodigal namesake, Jack (John Ames) Boughton, his best friends lost son who returns to Gilead searching for forgiveness and redemption. Told in John Amess joyous, rambling voice that finds beauty, humour and truth in the smallest of lifes details, Gilead is a song of celebration and acceptance of the best and the worst the world has to offer. At its heart is a tale of the sacred bonds between fathers and sons, pitch-perfect in style and story, set to dazzle critics and readers alike.\\', \\'Rate\\': 3.85}, {\\'Book\\': \\'Rage of angels\\', \\'Feedback\\': \"A memorable, mesmerizing heroine Jennifer -- brilliant, beautiful, an attorney on the way up until the Mafia\\'s schemes win her the hatred of an implacable enemy -- and a love more destructive than hate. A dangerous, dramatic world The Dark Arena of organized crime and flashbulb lit courtrooms where ambitious prosecutors begin their climb to political power.\", \\'Rate\\': 3.93}, {\\'Book\\': \\'Warhost of Vastmark\\', \\'Feedback\\': \"Tricked once more by his wily half-brother, Lysaer, Lord of Light, arrives at the tiny harbor town of Merior to find that Arithon\\'s ship yards have been abandoned and meticulously destroyed, and that the Master of Shadow has disappeared as if into thin air. Meanwhile Arithon and the Mad Prophet Dakar are traveling on foot through the treacherous Kelhorn Mountains towards the Vastmark clans, there to raise further support for his cause. But raising a warhost is a costly business. Is it mere coincidence that Princess TalithLysaer\\'s beautiful, headstrong wifeis taken captive and held for a vast ransom by a master brigand? The forces of light and shadow circle and feint, drawing ever closer to a huge conflict. And in the background the Fellowship of Seven Sorcerers and the Koriani Enchantresses watch and plan, and wait.\", \\'Rate\\': 4.03}, {\\'Book\\': \\'Taken at the Flood\\', \\'Feedback\\': \\'A Few Weeks After Marrying An Attractive Young Widow, Gordon Cloade Is Tragically Killed By A Bomb Blast In The London Blitz. Overnight, The Former Mrs Underhay Finds Herself In Sole Possession Of The Cloade Family Fortune. Shortly Afterwards, Hercule Poirot Receives A Visit From The Dead Man S Sister-In-Law Who Claims She Has Been Warned By Spirits That Mrs Underhay S First Husband Is Still Alive. Poirot Has His Suspicions When He Is Asked To Find A Missing Person Guided Only By The Spirit World. Yet What Mystifies Poirot Most Is The Woman S True Motive For Approaching Him&\\', \\'Rate\\': 3.71}, {\\'Book\\': \\'The illustrated man\\', \\'Feedback\\': nan, \\'Rate\\': 4.14}, {\\'Book\\': \"The Yiddish Policemen\\'s Union\", \\'Feedback\\': \\'For sixty years, Jewish refugees and their descendants have prospered in the Federal District of Sitka, a \"temporary\" safe haven created in the wake of revelations of the Holocaust and the shocking 1948 collapse of the fledgling state of Israel. Proud, grateful, and longing to be American, the Jews of the Sitka District have created their own little world in the Alaskan panhandle, a vibrant, gritty, soulful, and complex frontier city that moves to the music of Yiddish. For sixty years they have been left alone, neglected and half-forgotten in a backwater of history. Now the District is set to revert to Alaskan control, and their dream is coming to an end: once again the tides of history threaten to sweep them up and carry them off into the unknown. But homicide detective Meyer Landsman of the District Police has enough problems without worrying about the upcoming Reversion. His life is a shambles, his marriage a wreck, his career a disaster. He and his half-Tlingit partner, Berko Shemets, can\\\\\\'t catch a break in any of their outstanding cases. Landsman\\\\\\'s new supervisor is the love of his lifeand also his worst nightmare. And in the cheap hotel where he has washed up, someone has just committed a murderright under Landsman\\\\\\'s nose. Out of habit, obligation, and a mysterious sense that it somehow offers him a shot at redeeming himself, Landsman begins to investigate the killing of his neighbor, a former chess prodigy. But when word comes down from on high that the case is to be dropped immediately, Landsman soon finds himself contending with all the powerful forces of faith, obsession, hopefulness, evil, and salvation that are his heritageand with the unfinished business of his marriage to Bina Gelbfish, the one person who understands his darkest fears. At once a gripping whodunit, a love story, an homage to 1940s noir, and an exploration of the mysteries of exile and redemption, The Yiddish Policemen\\\\\\'s Union is a novel only Michael Chabon could have written.\\', \\'Rate\\': 3.7}, {\\'Book\\': \\'The Silmarillion\\', \\'Feedback\\': \\'Designed to take fans of The Hobbit and The Lord of the Rings deeper into the myths and legends of Middle-earth The Silmarillion is an account of the Elder Days, of the First Age of Tolkien s world. It is the ancient drama to which the characters in The Lord of the Rings look back, and in whose events some of them such as Elrond and Galadriel took part. The tales of The Silmarillion are set in an age when Morgoth, the first Dark Lord, dwelt in Middle-Earth, and the High Elves made war upon him for the recovery of the Silmarils, the jewels containing the pure light of Valinor. Included in the book are several shorter works. The Ainulindale is a myth of the Creation and in the Valaquenta the nature and powers of each of the gods is described. The Akallabeth recounts the downfall of the great island kingdom of Numenor at the end of the Second Age and Of the Rings of Power tells of the great events at the end of the Third Age, as narrated in The Lord of the Rings. This pivotal work features the revised, corrected text and includes, by way of an introduction, a fascinating letter written by Tolkien in 1951 in which he gives a full explanation of how he conceived the early Ages of Middle-earth.\"\\', \\'Rate\\': 3.91}, {\\'Book\\': \\'Tropic of Cancer\\', \\'Feedback\\': \"Miller\\'s groundbreaking first novel, banned in Britain for almost thirty years, now reinvigorated in a new Harper Perennial Modern Classics edition. A penniless and as yet unpublished writer, Henry Miller arrived in Paris in 1930. Leaving behind a disintegrating marriage and an unhappy career in America, he threw himself into the low-life of bohemian Paris with unwavering gusto. A fictional account of Miller\\'s adventures amongst the prostitutes and pimps, the penniless painters and writers of Montparnasse, Tropic of Cancer is an extravagant and rhapsodic hymn to a world of unrivalled eroticism and freedom. Tropic of Cancer\\'s 1934 publication in France was hailed by Samuel Beckett as \\'a momentous event in the history of modern writing\\'. The novel was subsequently banned in the UK and the USA and not released for publication for a further thirty years.\", \\'Rate\\': 3.69}, {\\'Book\\': \\'The Love of the Last Tycoon\\', \\'Feedback\\': \\'Depicts the inner-workings of the Hollywood movie industry and its impact on the fabric of American life.\\', \\'Rate\\': 3.65}, {\\'Book\\': \\'The Song of Rhiannon\\', \\'Feedback\\': \\'A retelling of The Mabinogion in novel form. Manawydon finally unites with Rhiannon - an aspect of the Goddess - but his happiness is shaken by the appearance of the Gray Man, who seeks vengeance against the living and especially against one who would claim the Goddess.\\', \\'Rate\\': 4.02}, {\\'Book\\': \\'Ocean Star Express\\', \\'Feedback\\': \"Joe and his parents are enjoying a summer holiday by the sea at the Ocean Star Hotel. The sky is bright blue, the sun shines and Joe loves all that the seaside has to offer. But when the fog rolls in and rain falls Joe begins to wish that he was back at home again. Things change, however, when the owner of the hotel invites Joe to share in a magical world, only a few steps away. The loft is black as night but then above Joe\\'s head a thousand tiny stars begin to sparkle and in the distance he hears the chug-chug-chug of a model train. A whole world is soon to open up before Joe\\'s eyes, a world of snow-capped mountains, great deserts, and rocking fishing boats.\", \\'Rate\\': 3.5}, {\\'Book\\': \\'The Princess of the Chalet School\\', \\'Feedback\\': nan, \\'Rate\\': 4.1}, {\\'Book\\': \\'The voyage of the Dawn Treader\\', \\'Feedback\\': \\'The \"Dawn Treader\" is the first ship Narnia has seen in centuries. King Caspian has built it for his voyage to find the seven lords, good men whom his evil uncle Mizaz banished when he usurped the throne. The journey takes Edmund, Lucy, and their cousin Eustace to the Eastern Islands, beyond the Silver Sea, toward Aslan\\\\\\'s country at the End of the World. Illustrations.\\', \\'Rate\\': 4.09}, {\\'Book\\': \\'Rest, Rabbit, Rest\\', \\'Feedback\\': \"Rabbit\\'s schedule keeps him so busy his friends have to trick him into resting.\", \\'Rate\\': 4.01}, {\\'Book\\': \\'Where the Red Fern Grows\\', \\'Feedback\\': \"A young boy living in the Ozarks achieves his heart\\'s desire when he becomes the owner of two redbone hounds and teaches them to be champion hunters.\", \\'Rate\\': 4.37}, {\\'Book\\': \"Poppy\\'s Return\", \\'Feedback\\': \"There\\'s trouble at Gray House, the girlhood home that Poppy left long ago. Poppy\\'s family has called her back to save them allmother, father, sisters and brothers, and dozens and dozens of deer mouse cousins. Poppy invites her rebellious son, Junior, to join her on the long trip across Dimwood Forest, hoping the journey will bring them closer together. But with Junior\\'s skunk pal, Mephitis, and Ereth, the cantankerous porcupine, in towsugared slug soup!Poppy and Junior may be in for unexpected adventure.\", \\'Rate\\': 3.99}, {\\'Book\\': \\'Diary of a Spider\\', \\'Feedback\\': \"This is the diary ... of a spider. But don\\'t be worried  he\\'s more scared of you and your gigantic shoe! Actually, he\\'s a lot like you. He goes to gym class and has Grandparents\\' Day at school. But he also spins sticky webs, scales walls, and takes windcatching lessons. Lucky for him, his best friend is a fly! Doreen Cronin and Harry Bliss, the team behind the #1 bestselling Diary Of A Worm, spin a hilarious tale about the upsidedown web world of an eightlegged charmer and his unlikely friend, Fly.\", \\'Rate\\': 4.25}, {\\'Book\\': \\'An Old-Fashioned Thanksgiving\\', \\'Feedback\\': \\'An adaptation of the original story follows the activities of seven children in nineteenth-century New England as they prepare for the Thanksgiving holiday while Mother is away caring for Grandmother.\\', \\'Rate\\': 3.7}, {\\'Book\\': \\'The Amazing Maurice and His Educated Rodents\\', \\'Feedback\\': \"Winner of the 2001 Carnegie Medal One rat, popping up here and there, squeaking loudly, and taking a bath in the cream, could be a plague all by himself. After a few days of this, it was amazing how glad people were to see the kid with his magical rat pipe. And they were amazing when the rats followed hint out of town. They\\'d have been really amazed if they\\'d ever found out that the rats and the piper met up with a cat somewhere outside of town and solemnly counted out the money. The Amazing Maurice runs the perfect Pied Piper scam. This streetwise alley cat knows the value of cold, hard cash and can talk his way into and out of anything. But when Maurice and his cohorts decide to con the town of Bad Blinitz, it will take more than fast talking to survive the danger that awaits. For this is a town where food is scarce and rats are hated, where cellars are lined with deadly traps, and where a terrifying evil lurks beneath the hunger-stricken streets.... Set in Terry Pratchett\\'s widely popular Discworld, this masterfully crafted, gripping read is both compelling and funny. When one of the world\\'s most acclaimed fantasy writers turns a classic fairy tale on its head, no one will ever look at the Pied Piper -- or rats -- the same way again!\", \\'Rate\\': 4.05}, {\\'Book\\': \\'The Wee Free Men\\', \\'Feedback\\': \\'A nightmarish danger threatens from the other side of reality . . . Armed with only a frying pan and her common sense, young witch-to-be Tiffany Aching must defend her home against the monsters of Fairyland. Luckily she has some very unusual help: the local Nac Mac Feegleaka the Wee Free Mena clan of fierce, sheep-stealing, sword-wielding, six-inch-high blue men. Together they must face headless horsemen, ferocious grimhounds, terrifying dreams come true, and ultimately the sinister Queen of the Elves herself. . . . A Story of Discworld\\', \\'Rate\\': 4.27}, {\\'Book\\': \\'The Real Trial of Oscar Wilde\\', \\'Feedback\\': \\'Oscar Wilde had one of literary history\\\\\\'s most explosive love affairs with Lord Alfred \"Bosie\" Douglas. In 1895, Bosie\\\\\\'s father, the Marquess of Queensberry, delivered a note to the Albemarle Club addressed to \"Oscar Wilde posing as sodomite.\" With Bosie\\\\\\'s encouragement, Wilde sued the Marquess for libel. He not only lost but he was tried twice for \"gross indecency\" and sent to prison with two years\\\\\\' hard labor. With this publication of the uncensored trial transcripts, readers can for the first time in more than a century hear Wilde at his most articulate and brilliant. The Real Trial of Oscar Wilde documents an alarmingly swift fall from grace; it is also a supremely moving testament to the right to live, work, and love as one\\\\\\'s heart dictates.\\', \\'Rate\\': 4.04}, {\\'Book\\': \\'A Year in the Life of William Shakespeare\\', \\'Feedback\\': \\'1599 was an epochal year for Shakespeare and England Shakespeare wrote four of his most famous plays: Henry the Fifth, Julius Caesar, As You Like It, and, most remarkably, Hamlet; Elizabethans sent off an army to crush an Irish rebellion, weathered an Armada threat from Spain, gambled on a fledgling East India Company, and waited to see who would succeed their aging and childless queen. James Shapiro illuminates both Shakespeares staggering achievement and what Elizabethans experienced in the course of 1599, bringing together the news and the intrigue of the times with a wonderful evocation of how Shakespeare worked as an actor, businessman, and playwright. The result is an exceptionally immediate and gripping account of an inspiring moment in history.\\', \\'Rate\\': 4.09}, {\\'Book\\': \\'Travels\\', \\'Feedback\\': \"Often I feel I go to some distant region of the world to be reminded of who I really am. When Michael Crichton -- a Harvard-trained physician, bestselling novelist, and successful movie director -- began to feel isolated in his own life, he decided to widen his horizons. He tracked wild animals in the jungles of Rwanda. He climbed Kilimanjaro and Mayan pyramids. He trekked across a landslide in Pakistan. He swam amid sharks in Tahiti. Fueled by a powerful curiosity and the need to see, feel, and hear firsthand and close-up, Michael Crichton has experienced adventures as compelling as those he created in his books and films. These adventures -- both physical and spiritual -- are recorded here in Travels, Crichton\\'s most astonishing and personal work.\", \\'Rate\\': 3.95}, {\\'Book\\': \\'Walt Whitman\\', \\'Feedback\\': \"Whitman\\'s genius, passions, poetry, and androgynous sensibility entwined to create an exuberant life amid the turbulent American mid-nineteenth century. In vivid detail, Kaplan examines the mysterious selves of the enigmatic man who celebrated the freedom and dignity of the individual and sang the praises of democracy and the brotherhood of man.\", \\'Rate\\': 3.99}, {\\'Book\\': \\'How to Make Love Like a Porn Star\\', \\'Feedback\\': \\'When the stewardess brought me off the plane in a wheelchair, I lowered my head. I was too scared to even look at my father. I didn\\\\\\'t want to see the disappointment and horror on his face. All that hate I had accumulated for him over the years, all the resentment against him for not understanding what I was going through, just released with the tears. \"So, where are your parents?\" the stewardess asked me after a few minutes. \"I can\\\\\\'t wait here with you much longer.\" I looked up and wiped my eyes. My father was standing ten feet away. He didn\\\\\\'t even recognize me. In the underbelly of Las Vegas, a cesspool of warring biker gangs and seedy strip clubs transformed the gawky, brace-faced Jenna Massoli into the bombshell Jenna Jameson. Today, Jenna Jameson is the biggest star in the history of adult movies, consistently ranked as one of the most beautiful women alive. But behind the glamour and the meteoric rise to fame was a path paved with tragedy and heartbreak. As a teenager drawn into a chaotic world ruled by rape, abuse, and murder, Jenna plunged into a downward spiral of addiction, even as she became one of the most photographed women in adult magazines. Determined to overcome this past, Jenna rebounded in the adult-film business, where she encountered sadistic directors, experienced lovers of both sexes, amorous celebrities (from Howard Stern to Marilyn Manson to Tommy Lee), bitter rival starlets, and finally, glory, as she went on to become the biggest porn star the world has ever seen. But her struggle for happiness did not end when the accolades began. For years she wrestled with her resentment at her estranged father, the loneliness of growing up from the age of two without a mother, and her enduring childhood desire to find a man who could give her the security and love she never had. Her unforgettable memoir is many things at once: a shocking sexual history, an insider\\\\\\'s guide to the secret workings of the billion-dollar adult-film industry, and a gripping thriller that probes deep into Jenna\\\\\\'s dark past. An unparalleled exploration of sexual freedom, How to Make Love Like a Porn Star ventures far beyond the flesh, into the heart-shredding tragedies and adrenaline-pumping triumphs of a woman who has already lived a hundred lifetimes. Always witty and humorous even as she faces the demons of her past, Jenna offers hilarious anecdotes about one of the most controversial businesses in history, and shares outrageous advice, including her ten commandments of dating and sex, how to become a \"suitcase pimp,\" and how to make it in the business as a female (or a male). Add to this never-before-seen photographs from Jenna\\\\\\'s private collection and others taken exclusively for this book, and the result is certain to be one of the most talked-about books of the year.\\', \\'Rate\\': 3.75}, {\\'Book\\': \\'The Bradbury Chronicles\\', \\'Feedback\\': \"Accomplished journalist Sam Weller met the Ray Bradbury while writing a cover story for the Chicago Tribune Magazine and spent hundreds of hours interviewing Bradbury, his editors, family members, and longtime friends. With unprecedented access to private archives, he uncovered neverbeforepublished letters, documents, and photographs that help tell the story of this literary genius and his remarkable creative journey. The result is a richly textured, detailed biography that illuminates the origins and accomplishments of Bradbury\\'s fascinating mind.\", \\'Rate\\': 4.22}, {\\'Book\\': \\'The Game\\', \\'Feedback\\': \"Hidden somewhere, in nearly every major city in the world, is an underground seduction lair. And in these lairs, men trade the most devastatingly effective techniques ever invented to charm women. This is not fiction. These men really exist. They live together in houses known as Projects. And Neil Strauss, the bestselling author, spent two years living among them, using the pseudonym Style to protect his real-life identity. The result is one of the most explosive and controversial books of the year -- guaranteed to change the lives of men and transform the way women understand the opposite sex forever. On his journey from AFC (average frustrated chump) to PUA (pick-up artist) to PUG (pick-up guru), Strauss not only shares scores of original seduction techniques but also has unforgettable encounters with the likes of Tom Cruise, Britney Spears, Paris Hilton, Heidi Fleiss, and Courtney Love. And then things really start to get strange -- and passions lead to betrayals lead to violence. The Game is the story of one man\\'s transformation from frog to prince -- to prisoner in the most unforgettable book of the year.\", \\'Rate\\': 3.74}, {\\'Book\\': \\'Ulysses S. Grant\\', \\'Feedback\\': \"One of the first two volumes in Harper\\'s Eminent Lives series, Korda brings his acclaimed storytelling talents to the life of Ulysses S. Grant  a man who managed to end the Civil War on a note of grace, serve two terms as president, write one of the most successful military memoirs in American literature, and is today remembered as a brilliant general but a failed president. Ulysses S. Grant was the first officer since George Washington to become a fourstar general in the United States Army, and the only president between Andrew Jackson and Woodrow Wilson to serve eight consecutive years in the White House. In this succinct and vivid biography, Michael Korda considers Grant\\'s character and reconciles the conflicting evaluations of his leadership abilities. Grant\\'s life played out as a true Horatio Alger story. Despite his humble background as the son of a tanner in Ohio, his lack of early success in the army, and assorted failed business ventures, his unwavering determination propelled him through the ranks of military leadership and into the presidency. But while the general\\'s tenacity and steadfastness contributed to his success on the battlefield, it both aided and crippled his effectiveness in the White House. Assessing Grant both within the context of his time and in contrast to more recent American leaders, Korda casts a benevolent eye on Grant\\'s presidency while at the same time conceding his weaknesses. He suggests that though the general\\'s second term ended in financial and political scandals, the fact remains that for eight years Grant exerted a calming influence on a country that had only just emerged from a horrendous civil war. Ulysses S. Grant is an evenhanded and stirring portrait of a man who guided America through a pivotal juncture in its history.\", \\'Rate\\': 3.88}, {\\'Book\\': \\'Thomas Jefferson\\', \\'Feedback\\': \"In this unique biography of Thomas Jefferson, leading journalist and social critic Christopher Hitchens offers a startlingly new and provocative interpretation of our Founding Father. Situating Jefferson within the context of America\\'s evolution and tracing his legacy over the past two hundred years, Hitchens brings the character of Jefferson to life as a man of his time and also as a symbolic figure beyond it. Conflicted by power, Jefferson wrote the Declaration of Independence and acted as Minister to France yet yearned for a quieter career in the Virginia legislature. Predicting that slavery would shape the future of America\\'s development, this professed proponent of emancipation elided the issue in the Declaration and continued to own human property. An eloquent writer, he was an awkward public speaker; a reluctant candidate, he left an indelible presidential legacy. Jefferson\\'s statesmanship enabled him to negotiate the Louisiana Purchase with France, doubling the size of the nation, and he authorized the Lewis and Clark expedition, opening up the American frontier for exploration and settlement. Hitchens also analyzes Jefferson\\'s handling of the Barbary War, a lesser-known chapter of his political career, when his attempt to end the kidnapping and bribery of Americans by the Barbary states, and the subsequent war with Tripoli, led to the building of the U.S. navy and the fortification of America\\'s reputation regarding national defense. In the background of this sophisticated analysis is a large historical drama: the fledgling nation\\'s struggle for independence, formed in the crucible of the eighteenth-century Enlightenment, and, in its shadow, the deformation of that struggle in the excesses of the French Revolution. This artful portrait of a formative figure and a turbulent era poses a challenge to anyone interested in American history -- or in the ambiguities of human nature.\", \\'Rate\\': 3.92}, {\\'Book\\': \\'Muhammad\\', \\'Feedback\\': \"Muhammad was born in 570 CE, and over the following sixty years built a thriving spiritual community, laying the foundations of a religion that changed the course of world history. There is more historical data on his life than on that of the founder of any other major faith, and yet his story is little known. Karen Armstrong\\'s immaculately researched new biography of Muhammad will enable readers to understand the true origins and spirituality of a faith that is all too often misrepresented as cruel, intolerant, and inherently violent. An acclaimed authority on religious and spiritual issues, Armstrong offers a balanced, in-depth portrait, revealing the man at the heart of Islam by dismantling centuries of misconceptions. Armstrong demonstrates that Muhammad\\'s lifea pivot point in historyhas genuine relevance to the global crises we face today.\", \\'Rate\\': 4.04}, {\\'Book\\': \\'Spandau\\', \\'Feedback\\': nan, \\'Rate\\': 4.12}]'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chatbot_response('Recommend a book in History')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bvKqkkRLQwD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laPMq54hLQzQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6zNnvBuLQ2W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTQK-LNOLQ5Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
